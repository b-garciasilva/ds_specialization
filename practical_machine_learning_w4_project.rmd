---
title: "DataScienceCousera - W4 Project"
author: "Bruno Garcia da Silva"
date: "4/11/2021"
output:
  html_document: default
  pdf_document: default
---

## Coursera Practical Machine Learning - Course Project
### Human Activity Recognition

## Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will us the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways, reflected on the variable **classe**, which contains the following levels:

Class A: exactly according to the specification
Class B: throwing the elbows to the front
Class C: lifting the dumbbell only halfway
Class D: lowering the dumbbell only halfway
Class E: throwing the hips to the front


```{r Load Required Libraries, include=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(gbm)
library(corrplot)

## For Improving GBM MOdel Execution
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
number = 5,
allowParallel = TRUE)

set.seed(202)
```

This analysis will be based on the data available by the researchers on: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. The dataset consists of a training set and a test set that will be later used to evaluate the models created. 

```{r Load the Data, echo=TRUE}
train_ds_raw <- read.csv(
    file=url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
    )
test_ds_raw<- read.csv(
  file=url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
  )
```

The training dataset consists of `r nrow(train_ds_raw)` rows and  `r length(train_ds_raw)` variables.
The test dataset consists of `r nrow(test_ds_raw)` rows and  `r length(test_ds_raw)` variables.

```{r Describing the Data, echo=TRUE}
## Analyzing the Train Dataset
str(train_ds_raw, list.len=30)
str(test_ds_raw, list.len = 30)
```

As can be observed by the summary the dataset contains a significant number of variables with null and blank values, which would not add to our analysis. Therefore we will have to clean the dataset by:

1. Removing variables with Near Nero Variance from our dataset.
```{r Transforming the Training Dataset, echo=TRUE}
## Removing NearZeroVariance variables

train_dsNZV <- nearZeroVar(train_ds_raw)
train_ds_raw <- train_ds_raw[,-train_dsNZV]
test_ds_raw  <- test_ds_raw[,-train_dsNZV]
```

2. Removing variables having  all values as N/A.
```{r Cleaning the Variables, echo=TRUE}
## Removing N/A's
train_ds_raw <- train_ds_raw[, colSums(is.na(train_ds_raw)) == 0]
test_ds_raw <- test_ds_raw[, colSums(is.na(test_ds_raw)) == 0]

```


3. Removing non-numeric variables that will not be used in our analysis (such as date and labels)
```{r Remove Unecessary Fields, echo=TRUE}
train_ds_raw <- train_ds_raw[,-(1:6)]
test_ds_raw <- test_ds_raw[,-(1:6)]
```

With that our new dataset now has the following dimensions:

The training dataset after adjustments consists of `r nrow(train_ds_raw)` rows and  `r length(train_ds_raw)` variables.
The test dataset after adjustments consists of `r nrow(test_ds_raw)` rows and  `r length(test_ds_raw)` variables.


## Correlation Analysis

A correlation analysis is performed on our training dataset to understand how the variables relates with each other and  how that could impact our analysis. The variables with higher correlation have darker colors in the graph below. A Principal Component Analysis could be performed to optimize variables selection, but as there are few variables strongly correlated in our datasets we won't perform such analysis.

```{r correlation, echo=FALSE}
corMatrix <- cor(train_ds_raw[-53])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

## Model Building

To start creating the models we will first split our training dataset, having 75% used for training the models and 25% to be used as a cross validation.

```{r Cross Validation, echo=FALSE}
  
## Cross Validation
inTrain <- createDataPartition(train_ds_raw$classe, p=0.75, list=FALSE)
training <- train_ds_raw[inTrain,]
crossValid <- train_ds_raw[-inTrain,]

dim(training)
```

### Applying ML Models
We will start applying different machine learning techiniques to find the one with best accuracy on identifying the correct **classe** result for each observation. The techiniques used in this analysis will be Decison Trees, Random Forests and Gradient Boost. After applying the model to the cross validation dataset a confusion matrix will be created to assess the accuracy of the model.

#### a) Decision Trees
```{r decision trees, echo=TRUE}
modDecisionTree <- train(classe ~ ., data = training, method="rpart")
```

```{r decision trees chart, echo=FALSE}
rpart.plot(modDecisionTree$finalModel, roundint=FALSE)
```

```{r decision trees results, echo=FALSE}
predictDecTree <- predict(modDecisionTree, crossValid)
dec_tree_cm <- confusionMatrix(factor(crossValid$classe),predictDecTree)
dec_tree_cm
```

The decision tree does not show a good accuracy, being able to identify `r round(dec_tree_cm$overall[1]* 100,4)`% of the observations correctly.

#### b) Random Forests

```{r random_forest_model, echo=TRUE}
mod_random_forest<-train(classe ~ ., data=training ,method="rf", ntree = 100)
```

```{r random_forest_prediction , echo=FALSE}
random_forest_results <- predict(mod_random_forest, crossValid)
random_forest_confusion_matrix <- confusionMatrix(random_forest_results, factor(crossValid$classe))
random_forest_confusion_matrix
```

The random forest model shows a good accuracy, being able to identify `r round(random_forest_confusion_matrix$overall[1]* 100,4)`% of the observations correctly.

```{r random_forest_results_plot , echo=FALSE}
plot(random_forest_confusion_matrix$table, col = random_forest_confusion_matrix$byClass, 
     main = paste("Random Forest Model - Accuracy Level =",
                  round(random_forest_confusion_matrix$overall['Accuracy'], 4)))
```

#### c) Gradient Boosting Model (GBM)

```{r gbm_model , echo=TRUE}
#Using trControl to add parallelism and speed up the execution
model_gbm <-train(classe ~ ., data=training,method="gbm", verbose = FALSE,trControl = fitControl)
```

```{r stop paralellism , echo=FALSE}
stopCluster(cluster)
registerDoSEQ()
```


```{r gbm_prediction , echo=FALSE}
prediction_gbm <- predict(model_gbm,crossValid)
gbm_confusion_matrix <- confusionMatrix(prediction_gbm, factor(crossValid$classe))
gbm_confusion_matrix
```

The gradient boosting model also shows a good accuracy, being able to identify `r round(gbm_confusion_matrix$overall[1]* 100,4)`% of the observations correctly.

```{r gbm_plot , echo=FALSE}
plot(gbm_confusion_matrix$table, col = gbm_confusion_matrix$byClass, 
     main = paste("GBM - Accuracy Level =",
                  round(gbm_confusion_matrix$overall['Accuracy'], 4)))
```

## Conclusion

In conclusion, after evaluating each model we observed that the **random forest model** shows the best fit being able to predict with higher precision than other models the variable **classe** for each observation and therefore, will be used to predict the **classe** variable in our testing dataset.

```{r Best Model, echo=FALSE}
model_evaluation <- rbind(c("Decision Tree Model",dec_tree_cm$overall[1]),
                          c("Random Forest Model",random_forest_confusion_matrix$overall[1]),
                          c("Gradient Boosting Model",gbm_confusion_matrix$overall[1])
                          )
model_evaluation
```


## Predicting on the test data

```{r Predicting on the test data, echo=TRUE}
bestModel <- mod_random_forest
predict <- predict(bestModel, newdata=test_ds_raw)
predict
```
